WHAT IS THIS?

This is a parallel implementation of L1-regularized logistic regression using
the alternating direction method of multipliers scheme. It follows the
formulation given in Boyd et al. (2010), with a slight modification to
incorporate intercepts.

COMPILATION

The program relies on OpenMPI (mpi.h, mpic++, mpirun) and the dlib C++ library.
dlib (http://dlib.net) must be installed to compile using the Makefile
provided. Once dlib is downloaded, simply set DLIB equal to your dlib install
directory, then make.

INPUT DATA

The data you wish to fit MUST be standardized. If not, convergence is not
assured. Given N by p data matrix X and target column vector y, let
M = [ 1, X, y], where the first element is a column of all ones. Let
"data.txt" be a space- or tab-delimited text file containing M.

Given k processing elements, the program expects at least k-1 separate
data files, called "data_1.txt", "data_2.txt", ..., "data_{k-1}.txt",
where all files are of precisely the same dimensions. If your original data
is stored in "data.txt", then "data_1.txt", ..., "data_{k-1}.txt" are simply
k-1 chunks of the original matrix M, each of dimension N/(k-1) by p+2.

IMPORTANT: the dimensions of the matrices contained in the input files MUST
be the same across all files. Also, if there are more than k-1 input files,
only files 1 through k-1 will be processed.

USAGE

Once you've compiled the program and your data files are located in the
local folder, you're ready to go. To execute, do:

mpirun -n k mpi_admm N/(k-1) p iters rho lambda

where k in the number of processing elements you're using, N and p are the
dimensions of X, iters is the number of iterations you'd like to perform,
rho is the penalty parameter of the augmented Lagrangian, and lambda is the
L1-regularization penalty term. Tuning of rho and lambda is definitely
required! In general, expect to use small values (on the order of 0.01) when
fitting data with hundreds or more variables.

TEST DATA

In the "bigdata" directory I've included some test data synthesized using
the vector in "bigM_true_beta.txt" as the original coefficients. To run the
program on this data, simply move it into your local folder, rename the files
as "data_i.txt" for i = 1,2,3,4, and run it with 5 processors. If you only
have four processors, you can do mpirun -n 4, but chunk 4 won't be processed.
About the "true beta": the original motivation behind this project was to look at
the effectiveness of the ADMM L1-regularized logistic regression at performing
variable selection, which is why beta has such a simple form (binary, with few
non-zero entries). As can be seen from the example output obtained by running
on the test data, it performs variable selection tolerably well.

GOING FORWARD

Though this program was for a class project, I plan to continue to tinker with
it. I also plan to test it on larger-scale, more complicated datasets and make
corresponding improvements. If you have any suggestions or can think of
alterations I could make that would render it useful to you, please let me know.

Wes Suttle at Stony Brook University, fall 2018.
